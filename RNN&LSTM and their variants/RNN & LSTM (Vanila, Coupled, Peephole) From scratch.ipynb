{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkGQCQC4fSL9YQoDDbLX3F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZKIgdWPvox1","executionInfo":{"status":"ok","timestamp":1747320938227,"user_tz":-180,"elapsed":20,"user":{"displayName":"Mohammad Elsharqawy","userId":"15168035335048756178"}},"outputId":"94381ec0-67ad-475a-8d26-b1f25ba7dce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["import torch, datasets, math\n","import torch.nn as nn\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","source":["class RNN_cell(nn.Module):\n","  def __init__(self, input_dim, hidden_dim):\n","    super().__init__()\n","\n","    self.hidden_dim = hidden_dim\n","\n","    self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","    self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","    self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","    self.init_weights()\n","\n","  def init_weights(self):\n","        #If hidden_dim = 16, then stdv = 1 / sqrt(16) = 0.25, so weights are sampled from [-0.25, 0.25].\n","    std = 1 / math.sqrt(self.hidden_dim)\n","\n","    for weight in self.parameters():\n","        weight.data.uniform_(-std, std)\n","\n","  def forward(self, x, init_state=None):\n","\n","    bs, seq_len,_ = x.shape\n","    output = []\n","\n","\n","    if init_state is None:\n","        h_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n","    else:\n","      h_t = init_state\n","\n","\n","    for t in range(seq_len):\n","      x_t = x[:,t,:] # all sequences at time step t\n","\n","      h_t = torch.tanh(  h_t @ self.W_g + x_t @ self.U_g + self.b_g) #bs, hidden\n","\n","      output.append(h_t.unsqueeze(0)) #(1, bs, hidden)\n","\n","    output = torch.cat(output, dim=0)\n","    output = output.transpose(0, 1).contiguous()\n","\n","    return output, h_t"],"metadata":{"id":"zYzKF_iJV2fS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTM_cell(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","      super().__init__()\n","\n","      self.hidden_dim = hidden_dim\n","\n","      #Trainable parameters\n","      self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","      self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","      self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","      self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","      self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","      self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","      self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","      self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","      self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","      self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n","      self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","      self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","      self.init_weights()\n","\n","\n","    def init_weights(self):\n","      #If hidden_dim = 16, then stdv = 1 / sqrt(16) = 0.25, so weights are sampled from [-0.25, 0.25].\n","      std = 1 / math.sqrt(self.hidden_dim)\n","\n","      for weight in self.parameters():\n","          weight.data.uniform_(-std, std)\n","\n","\n","    def forward(self, x, init_state=None):\n","\n","      bs, seq_len, _ = x.shape\n","      output = []\n","\n","      if init_state is None:\n","        h_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n","        c_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n","\n","      else:\n","        h_t, c_t = init_state\n","\n","      for t in range(seq_len):\n","          x_t = x[:,t,:] # all sequences at time step t\n","\n","          f_t = torch.sigmoid( h_t@ self.W_f + x_t @ self.U_f + self.b_f)\n","          i_t = torch.sigmoid( h_t@ self.W_i + x_t @ self.U_i + self.b_i)\n","          o_t = torch.sigmoid( h_t@ self.W_o + x_t @ self.U_o + self.b_o)\n","\n","          g_t = torch.tanh(h_t@ self.W_g + x_t @ self.U_g + self.b_g)\n","\n","          c_t = (f_t * c_t) + (i_t * g_t)\n","\n","          h_t = o_t * torch.tanh(c_t)\n","\n","          output.append(h_t.unsqueeze(0)) #h_t -> (1, batch_size, hidden_dim)\n","\n","      output = torch.cat(output, dim=0)\n","      output = output.transpose(0, 1).contiguous()\n","\n","      return output, (h_t, c_t)\n"],"metadata":{"id":"jgfbfUMvxXQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#some hyperparameters\n","input_dim = 5000  #just for example\n","hidden_dim = 256\n","embed_dim = 300\n","output_dim = 1\n","\n","batch_size = 32"],"metadata":{"id":"vKt_OyS4cSJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_RNN_cell = RNN_cell(embed_dim, hidden_dim).to(device)\n","\n","test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n","output, h_t = my_RNN_cell(test_data)\n","\n","assert output.shape == torch.Size([32, 100, 256])\n","assert h_t.shape    == torch.Size([32, 256])"],"metadata":{"id":"Bvod3IZgh92o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_LSTM_cell = LSTM_cell(embed_dim, hidden_dim).to(device)\n","\n","test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n","output, (h_t, c_t) = my_LSTM_cell(test_data)\n","\n","assert output.shape == torch.Size([32, 100, 256])\n","assert h_t.shape    == torch.Size([32, 256])\n","assert c_t.shape    == torch.Size([32, 256])"],"metadata":{"id":"KI6-9ciSb-ru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1.2 Peephole LSTM**\n","\n","1.2 Peephole LSTM\n","One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “Peephole Connections.” This means that we let all the gate layers look at the cell state.\n","\n","\n","**Coupled LSTM**\n","\n","\n","Another variation is to use Coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. The difference is very simple. The input gate is now (1 - ft )"],"metadata":{"id":"aOJFUZc9pGHF"}},{"cell_type":"code","source":["class new_LSTM_cell(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, lstm_type):\n","     super().__init__()\n","\n","\n","     self.hidden_dim = hidden_dim\n","     self.lstm_type = lstm_type\n","\n","\n","     self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim ))\n","     self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","     self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","     self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim ))\n","     self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","     self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","     self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim ))\n","     self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","     self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","     self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim ))\n","     self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","     self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n","\n","\n","     if self.lstm_type == 'Peephole':\n","          self.P_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","          self.P_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","          self.P_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n","\n","\n","     self.init_weights()\n","\n","  def init_weights(self):\n","    std = 1/math.sqrt(self.hidden_dim)\n","\n","    for weight in self.parameters():\n","        weight.data.uniform_(-std, std)\n","\n","\n","  def forward(self, x, init_state=None):\n","    bs, seq_len, _ = x.shape\n","    output = []\n","\n","    if init_state is None:\n","      h_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n","      c_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n","    else:\n","      h_t, c_t = init_state\n","\n","\n","\n","    for t in range(seq_len):\n","      x_t = x[:,t,:]\n","\n","      if self.lstm_type in ['Vanilla', 'Coupled']:\n","        f_t = torch.sigmoid( h_t  @ self.W_f + x_t  @ self.U_f + self.b_f )\n","        o_t = torch.sigmoid( h_t @ self.W_o + x_t @ self.U_o + self.b_o)\n","\n","        if self.lstm_type == 'Vanilla':\n","          i_t = torch.sigmoid( h_t  @ self.W_i + x_t  @ self.U_i + self.b_i )\n","        if self.lstm_type == 'Coupled':\n","          i_t = (1 - f_t)\n","\n","      if self.lstm_type == 'Peephole':\n","        f_t = torch.sigmoid( h_t @ self.W_f + x_t @ self.U_f + c_t @ self.P_f + self.b_f)\n","        o_t = torch.sigmoid( h_t @ self.W_o + x_t @ self.U_o + c_t @ self.P_o + self.b_o)\n","        i_t = torch.sigmoid( h_t @ self.W_i + x_t @ self.U_i + c_t @ self.P_i + self.b_i)\n","\n","\n","\n","      g_t = torch.tanh(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n","\n","      c_t = (i_t * g_t) + (f_t * c_t)\n","      h_t = o_t * torch.tanh(c_t)\n","\n","\n","      output.append(h_t.unsqueeze(0)) # 1, bs, hidden_dim\n","\n","\n","    output = torch.cat(output, 0)\n","    output = output.transpose(0, 1).contiguous()\n","\n","\n","    return output, (h_t, c_t)\n"],"metadata":{"id":"8HXvJC_npIln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Vanilla_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'Vanilla').to(device)\n","test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n","output, (h_t, c_t) = Vanilla_LSTM_cell(test_data)\n","assert output.shape == torch.Size([32, 100, 256])\n","assert h_t.shape    == torch.Size([32, 256])\n","assert c_t.shape    == torch.Size([32, 256])\n","\n","Coupled_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'Coupled').to(device)\n","test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n","output, (h_t, c_t) = Coupled_LSTM_cell(test_data)\n","assert output.shape == torch.Size([32, 100, 256])\n","assert h_t.shape    == torch.Size([32, 256])\n","assert c_t.shape    == torch.Size([32, 256])\n","\n","Peephole_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'Peephole').to(device)\n","test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n","output, (h_t, c_t) = Peephole_LSTM_cell(test_data)\n","assert output.shape == torch.Size([32, 100, 256])\n","assert h_t.shape    == torch.Size([32, 256])\n","assert c_t.shape    == torch.Size([32, 256])"],"metadata":{"id":"ZARTJ5b_yqxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3r8NLPyAzOO8"},"execution_count":null,"outputs":[]}]}